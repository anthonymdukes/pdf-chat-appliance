# PDF Chat Appliance

A lightweight, self-hosted AI assistant that runs entirely on CPU and lets you query large PDFs using natural language.

---

## üß± Tech Stack

- Ubuntu Server 24.04 (OVA ready)
- Ollama (LLM inference engine)
- Open WebUI (chat interface)
- llama-index + ChromaDB (document indexing)
- CPU-only setup ‚Äî no GPU required

---

## üìÅ How to Use

### Using Python

1. (Optional) Create a virtual environment:
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```
2. Drop PDFs into the `documents/` folder.
3. Build the vector store:
   ```bash
   python3 load_all.py
   ```
4. Start the query server:
   ```bash
   python3 query_server.py
   ```
5. Access the web UI at: `http://<host>:3000`
6. Query the API with curl:
   ```bash
   curl -X POST http://<host>:5000/query -H 'Content-Type: application/json' \
        -d '{"question": "What is this document about?"}'
   ```

### Using Docker

1. Build and start containers:
   ```bash
   docker compose up --build
   ```
2. Place PDFs in `documents/` and rebuild the index:
   ```bash
   docker compose exec backend python load_all.py
   ```
3. Query the server as shown above.

To add new PDFs later, drop them into `documents/` and rerun `load_all.py` to
update the vector database.

---

## üîÅ Optional Enhancements

* Add Samba share to expose `/documents`
* Add cron or inotify trigger to auto-load new PDFs
* Deploy as OVA from Proxmox or VirtualBox

